{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Marco Lavorini - Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset\n",
    "Vito,Saverio. (2016). Air Quality. UCI Machine Learning Repository. https://doi.org/10.24432/C59K5F."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Input\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "air_quality = fetch_ucirepo(id=360) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = air_quality.data.features \n",
    "y = air_quality.data.targets \n",
    "  \n",
    "# metadata \n",
    "print(air_quality.metadata) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns except for the target column \n",
    "df = X.drop(['Date','Time','CO(GT)', 'NMHC(GT)', 'NOx(GT)', 'NO2(GT)', 'T', 'RH', 'AH'], axis=1)\n",
    "df = df[['PT08.S1(CO)', 'PT08.S2(NMHC)', 'PT08.S3(NOx)', 'PT08.S4(NO2)', 'PT08.S5(O3)', 'C6H6(GT)']]\n",
    "# the null values are marked as '-200'\n",
    "df = df[df['C6H6(GT)'] != -200]\n",
    "df = df[df['PT08.S1(CO)'] != -200]\n",
    "df = df[df['PT08.S2(NMHC)'] != -200]\n",
    "df = df[df['PT08.S3(NOx)'] != -200]\n",
    "df = df[df['PT08.S4(NO2)'] != -200]\n",
    "df = df[df['PT08.S5(O3)'] != -200]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data splitting\n",
    "\n",
    "we decided to split the dataset, leaving a 20% of the data out, untouched, for testing. The remaining data was further divided in 80% for train and 20% for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test sets\n",
    "dataset = df.values\n",
    "\n",
    "train_size = int(len(dataset) * 0.8)\n",
    "test_size = len(dataset) - train_size\n",
    "validation_size = int(train_size * 0.2)\n",
    "train_size = train_size - validation_size\n",
    "\n",
    "print(f'train: {train_size}, validation: {validation_size}, test: {test_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data was scaled only on the true train test, without the validation or test set. This is done to fit the scaler only on the training set without the 'future' data which in a real scenario should not be known."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the dataset\n",
    "X_ = dataset[:, :-1]\n",
    "y_ = dataset[:, -1]\n",
    "train_X, train_y = X_[:train_size], y_[:train_size]\n",
    "validation_X, validation_y = X_[train_size:train_size+validation_size], y_[train_size:train_size+validation_size]\n",
    "test_X, test_y = X_[train_size+validation_size:], y_[train_size+validation_size:]\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "train_X = scaler.fit_transform(train_X)\n",
    "validation_X = scaler.transform(validation_X)\n",
    "test_X = scaler.transform(test_X)\n",
    "\n",
    "scaler_y = MinMaxScaler(feature_range=(0, 1))\n",
    "train_y = scaler_y.fit_transform(train_y.reshape(-1, 1))\n",
    "validation_y = scaler_y.transform(validation_y.reshape(-1, 1))\n",
    "test_y = scaler_y.transform(test_y.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First model: LSTM\n",
    "\n",
    "We choose an LSTM neural network as the first model to predict the benzene, based on the PT08.* measurement. To create the model we used Keras which offers an easy LSTM layer implementation [2] in the standard Sequential model. The data is then reshaped accordingly to the documentation where it must have the shape (batch, timesteps, feature).\n",
    "\n",
    "[2] https://keras.io/api/layers/recurrent_layers/lstm/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_X.shape, train_y.shape, validation_X.shape, validation_y.shape, test_X.shape, test_y.shape)\n",
    "\n",
    "# reshape input to be [samples, time steps, features]\n",
    "train_X = np.reshape(train_X, (train_X.shape[0], 1, train_X.shape[1]))\n",
    "validation_X = np.reshape(validation_X, (validation_X.shape[0], 1, validation_X.shape[1]))\n",
    "test_X = np.reshape(test_X, (test_X.shape[0], 1, test_X.shape[1]))\n",
    "\n",
    "print(train_X.shape, train_y.shape, validation_X.shape, validation_y.shape, test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  Sequential()\n",
    "\n",
    "model.add(Input(shape=(train_X.shape[1], train_X.shape[2])))\n",
    "model.add(LSTM(16))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the first model\n",
    "\n",
    "It is important to notice that here the parameter 'shuffle' was set to False since we are working with a time-series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_X, train_y, validation_data=[validation_X, validation_y], epochs=15, batch_size=64, verbose=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rescaling the data back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict(test_X)\n",
    "predict = scaler_y.inverse_transform(predict)\n",
    "test_y = scaler_y.inverse_transform(test_y)\n",
    "rmse = math.sqrt(mean_squared_error(test_y, predict))\n",
    "print('Test RMSE: %.3f' % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data\n",
    "plt.plot(test_y, color = 'red', label = 'Real C6H6')\n",
    "plt.plot(predict, color = 'blue', label = 'Predicted C6H6', alpha=0.5)\n",
    "plt.title('C6H6 Prediction')\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('C6H6')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zooming on the first 300 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_y[0:300], color = 'red', label = 'Real C6H6')\n",
    "plt.plot(predict[0:300], color = 'blue', label = 'Predicted C6H6', alpha=0.5)\n",
    "plt.title('C6H6 Prediction')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('C6H6')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# second model\n",
    "\n",
    "Here we will use as inputs the current benzene value, and as output the one-step-ahead value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of the the dataset\n",
    "\n",
    "for each measurement of C6H6 we create the dataset $(\\text{C6H6}_t, \\text{C6H6}_{t+1})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2, y_2 = [], []\n",
    "for i in range(len(y_)-1):\n",
    "    X_2.append(y_[i])\n",
    "    y_2.append(y_[i+1])\n",
    "X_2 = np.array(X_2)\n",
    "y_2 = np.array(y_2)\n",
    "\n",
    "print(X_2.shape, y_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test sets\n",
    "dataset = df.values\n",
    "#calculate sizez\n",
    "train_size = int(len(X_2) * 0.8)\n",
    "test_size = len(X_2) - train_size\n",
    "validation_size = int(train_size * 0.2)\n",
    "train_size = train_size - validation_size\n",
    "#perform the split\n",
    "x_train, y_train = X_2[:train_size], y_2[:train_size]\n",
    "x_validation, y_validation = X_2[train_size:train_size+validation_size], y_2[train_size:train_size+validation_size]\n",
    "x_test, y_test = X_2[train_size+validation_size:], y_2[train_size+validation_size:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rescale the data as before, fitting the scaler only on the known data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = scaler.fit_transform(x_train.reshape(-1, 1))\n",
    "x_validation = scaler.transform(x_validation.reshape(-1, 1))\n",
    "x_test = scaler.transform(x_test.reshape(-1, 1))\n",
    "\n",
    "y_train = scaler.transform(y_train.reshape(-1, 1))\n",
    "y_validation = scaler.transform(y_validation.reshape(-1, 1))\n",
    "y_test = scaler.transform(y_test.reshape(-1, 1))\n",
    "\n",
    "\n",
    "print(x_train.shape, y_train.shape, x_validation.shape, y_validation.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the timestep needed for the LSTM layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.reshape(x_train, (x_train.shape[0], 1, x_train.shape[1]))\n",
    "\n",
    "x_validation = np.reshape(x_validation, (x_validation.shape[0], 1, x_validation.shape[1]))\n",
    "\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], 1, x_test.shape[1]))\n",
    "\n",
    "print(x_train.shape, y_train.shape, x_validation.shape, y_validation.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model #2, same hyperparameter for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 =  Sequential()\n",
    "\n",
    "model2.add(Input(shape=(x_train.shape[1], x_train.shape[2])))\n",
    "model2.add(LSTM(16))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Dense(1))\n",
    "\n",
    "model2.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train phase with the same settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model2.fit(x_train, y_train, validation_data=[x_validation, y_validation], epochs=15, batch_size=64, verbose=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for model #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict2 = model2.predict(x_test)\n",
    "\n",
    "predict2 = scaler.inverse_transform(predict2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate root mean squared error\n",
    "y_test = scaler.inverse_transform(y_test)\n",
    "rmse2 = math.sqrt(mean_squared_error(y_test, predict2))\n",
    "print('Test RMSE: %.3f' % rmse2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data\n",
    "plt.plot(y_test, color = 'red', label = 'Real C6H6')\n",
    "plt.plot(predict2, color = 'blue', label = 'Predicted C6H6', alpha=0.5)\n",
    "plt.title('C6H6 Prediction')\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('C6H6')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data\n",
    "plt.plot(y_test[:300], color = 'red', label = 'Real GT')\n",
    "plt.plot(predict2[:300], color = 'blue', label = 'Predicted GT', alpha=0.5)\n",
    "plt.title('GT Prediction')\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('GT')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our experimental setup we decided to use LSTM networks for both tasks. The data was splitted in a 80% for training/validation and the remaining 20% for testing, scaling accordingly based only on the data used for training and without shuffling to keep the data time-coherent. The performance of each model was assessed using RMSE.\n",
    "\n",
    "The first model, based on 4 sensor measurement of the PT08.* columns effectively captured the relationship between sensor data and benzene levels.\n",
    "\n",
    "The LSTM model designed for one-step-ahead prediction of benzene based on its current value had slightly lower (but still good) performance, suggesting that, while current benzene levels provide useful information, incorporating sensor measurements is a better choice.\n",
    "\n",
    "Performance Comparison:\n",
    "\n",
    "- The sensor-based prediction model outperformed the one-step-ahead model. Sensor data provides a better understanding of the factors influencing benzene levels, leading to more accurate predictions.\n",
    "- The one-step-ahead model, while slightly less accurate, still provides good results, and could be useful in scenarios where sensor data might not be available."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
